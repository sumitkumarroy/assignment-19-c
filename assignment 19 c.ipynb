{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee73202-bad3-4677-b04a-439e59a64baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The filter method in feature selection ranks features based on their staticalmeasurment and it worked independentaly of the model. this evelute the \\neach feature relevence to the target variable using centain criteria. like corelation matrix, chi-coure test, features are selected based on the ranking\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1\n",
    "'''The filter method in feature selection ranks features based on their staticalmeasurment and it worked independentaly of the model. this evelute the \n",
    "each feature relevence to the target variable using centain criteria. like corelation matrix, chi-coure test, features are selected based on the ranking\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7eea4c9-36e7-49d3-abe8-9a678a1ffbcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The wrapper method differ from filter method because it evalute feature subset based on model performence rather then using statical measurmant alone\\nit also involve training model with different combinations and selecting the one which yield the best performence/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "'''The wrapper method differ from filter method because it evalute feature subset based on model performence rather then using statical measurmant alone\n",
    "it also involve training model with different combinations and selecting the one which yield the best performence/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043f4e2a-3895-4a3a-9a51-2f5e78c0f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRegularization methods: Techniques like Lasso (L1 regularization) and Ridge (L2 regularization) that add penalties to the loss function to shrink less \\nimportant feature coefficients.\\nDecision Trees and Random Forests: Tree-based algorithms that inherently perform feature selection by splitting on the most important features.\\nGradient Boosting Machines: Methods like XGBoost or LightGBM that build multiple trees and rank features based on their contribution to the model's \\npredictive performance.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "'''\n",
    "Regularization methods: Techniques like Lasso (L1 regularization) and Ridge (L2 regularization) that add penalties to the loss function to shrink less \n",
    "important feature coefficients.\n",
    "Decision Trees and Random Forests: Tree-based algorithms that inherently perform feature selection by splitting on the most important features.\n",
    "Gradient Boosting Machines: Methods like XGBoost or LightGBM that build multiple trees and rank features based on their contribution to the model's \n",
    "predictive performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cb0006c-7904-45fa-8c24-2eadbd2cfd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDrawbacks of the Filter method include:\\n\\nIndependence from the model: It may not account for feature interactions and their combined effect on the model's performance.\\nSimplicity: Statistical measures used in the Filter method might overlook complex relationships between features and the target variable.\\nPotential for suboptimal feature sets: Since it evaluates features individually, the selected subset might not be the best for the specific model, \\npotentially leading to suboptimal performance.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "'''\n",
    "Drawbacks of the Filter method include:\n",
    "\n",
    "Independence from the model: It may not account for feature interactions and their combined effect on the model's performance.\n",
    "Simplicity: Statistical measures used in the Filter method might overlook complex relationships between features and the target variable.\n",
    "Potential for suboptimal feature sets: Since it evaluates features individually, the selected subset might not be the best for the specific model, \n",
    "potentially leading to suboptimal performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6013b98-1a0d-4de2-88c9-b9a74853c91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHandling large datasets: The computational efficiency of the Filter method makes it suitable for datasets with a large number of features.\\nQuick initial screening: It's useful for quickly identifying and removing irrelevant features before applying more computationally intensive methods.\\nLimited computational resources: The Filter method is less demanding in terms of computational power and time.\\nPreventing overfitting: By removing irrelevant features early, it helps reduce the risk of overfitting, especially in high-dimensional datasets.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "'''\n",
    "Handling large datasets: The computational efficiency of the Filter method makes it suitable for datasets with a large number of features.\n",
    "Quick initial screening: It's useful for quickly identifying and removing irrelevant features before applying more computationally intensive methods.\n",
    "Limited computational resources: The Filter method is less demanding in terms of computational power and time.\n",
    "Preventing overfitting: By removing irrelevant features early, it helps reduce the risk of overfitting, especially in high-dimensional datasets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b8fd8c-8259-417b-b715-d2b1e44c57d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo choose pertinent attributes using the Filter Method, follow these steps:\\n\\nPreprocessing: Clean the dataset by handling missing values and encoding categorical variables.\\nStatistical measures: Compute correlation coefficients, chi-square scores, or mutual information between each feature and the target variable (churn).\\nRank features: Rank the features based on their statistical scores.\\nThreshold selection: Set a threshold for feature selection, such as retaining the top 10 features or those with scores above a certain value.\\nSubset creation: Create a new dataset containing only the selected features.\\nModel evaluation: Build and evaluate the predictive model using the selected features to ensure they improve performance.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "'''\n",
    "To choose pertinent attributes using the Filter Method, follow these steps:\n",
    "\n",
    "Preprocessing: Clean the dataset by handling missing values and encoding categorical variables.\n",
    "Statistical measures: Compute correlation coefficients, chi-square scores, or mutual information between each feature and the target variable (churn).\n",
    "Rank features: Rank the features based on their statistical scores.\n",
    "Threshold selection: Set a threshold for feature selection, such as retaining the top 10 features or those with scores above a certain value.\n",
    "Subset creation: Create a new dataset containing only the selected features.\n",
    "Model evaluation: Build and evaluate the predictive model using the selected features to ensure they improve performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "839a4fc9-fe78-4fc2-bdcd-52a7a38bb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo use the Embedded method for feature selection in predicting soccer match outcomes:\\n\\nChoose an appropriate model: Select a model that incorporates feature selection, such as a decision tree, random forest, or Lasso regression.\\nTrain the model: Fit the model to the dataset, allowing it to identify important features during the training process.\\nExtract feature importance: For tree-based models, use feature importance scores; for Lasso, look at non-zero coefficients.\\nRank features: Rank the features based on their importance scores.\\nSelect top features: Retain the top-ranked features that contribute most to the model’s performance.\\nEvaluate performance: Validate the model using the selected features to ensure they enhance predictive accuracy.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "'''\n",
    "To use the Embedded method for feature selection in predicting soccer match outcomes:\n",
    "\n",
    "Choose an appropriate model: Select a model that incorporates feature selection, such as a decision tree, random forest, or Lasso regression.\n",
    "Train the model: Fit the model to the dataset, allowing it to identify important features during the training process.\n",
    "Extract feature importance: For tree-based models, use feature importance scores; for Lasso, look at non-zero coefficients.\n",
    "Rank features: Rank the features based on their importance scores.\n",
    "Select top features: Retain the top-ranked features that contribute most to the model’s performance.\n",
    "Evaluate performance: Validate the model using the selected features to ensure they enhance predictive accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6446bc8d-7a04-4595-a850-3597143f5296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo use the Wrapper method for selecting features in predicting house prices:\\n\\nDefine the model: Choose a suitable predictive model, such as linear regression or a decision tree.\\nFeature subset generation: Use techniques like forward selection, backward elimination, or recursive feature elimination (RFE) to generate different subsets of features.\\nModel training and evaluation: Train the model on each subset and evaluate its performance using metrics like RMSE or R^2.\\nSelect the best subset: Identify the feature subset that yields the best model performance.\\nCross-validation: Validate the selected subset using cross-validation to ensure robustness and prevent overfitting.\\nFinalize the model: Use the chosen feature subset to build the final predictive model.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "'''\n",
    "To use the Wrapper method for selecting features in predicting house prices:\n",
    "\n",
    "Define the model: Choose a suitable predictive model, such as linear regression or a decision tree.\n",
    "Feature subset generation: Use techniques like forward selection, backward elimination, or recursive feature elimination (RFE) to generate different subsets of features.\n",
    "Model training and evaluation: Train the model on each subset and evaluate its performance using metrics like RMSE or R^2.\n",
    "Select the best subset: Identify the feature subset that yields the best model performance.\n",
    "Cross-validation: Validate the selected subset using cross-validation to ensure robustness and prevent overfitting.\n",
    "Finalize the model: Use the chosen feature subset to build the final predictive model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed9b9c-f1df-4ee3-af6a-53cbab56d097",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
